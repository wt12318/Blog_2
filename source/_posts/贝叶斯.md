# 贝叶斯

[Bayes Rules! An Introduction to Applied Bayesian Modeling (bayesrulesbook.com)](https://www.bayesrulesbook.com/)

## Chapter 1 The Big (Bayesian) Picture

> We continuously update our knowledge about the world as we accumulate lived experiences, or  *collect data*

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/undefinedimage-20220907122612-1v71jdt.png)

**Bayesian and frequentist analyses share a common goal: to learn from data about the world around us.**

a, b, b, a 1+3+1+3=8

贝叶斯派和频率派对概率的解释不同：

* 贝叶斯派认为概率是测量事件的**相对合理性**
* 频率派认为概率是可重复事件进行长时间重复之后的**相对频率**

所以在掷硬币的实验中，贝叶斯派的人就会认为正反面是差不多的（物质的性质），而频率派认为如果反复掷一枚硬币，那么有 1/2 的硬币正面朝上。

贝叶斯在构建后验概率的时候实际在平衡先验与数据之间的权重，当数据收集的越来越多，我们的先验知识占的比重就会越来越少。这个对于贝叶斯知识构建也是重要的，当数据收集越来越多，对某一个问题就可能达成共识，即使两个人的先验知识不一样：

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/undefinedimage-20220926162220-tvv2cga.png)

> Specifically, a **Bayesian** analysis assesses the uncertainty of the hypothesis in light of the observed data, and a **frequentist** analysis assesses the uncertainty of the observed data in light of an assumed hypothesis.

**Asking questions**

* A Bayesian hypothesis test seeks to answer: In light of the observed data, what’s the chance that the hypothesis is correct?
* A frequentist hypothesis test seeks to answer: If in fact the hypothesis is incorrect, what’s the chance I’d have observed this, or even more extreme, data?

贝叶斯的假设检验是在观测到数据的情况下，假设是对的概率有多大；而频率派的假设检验是在假设是对的情况下，我们观测到这样或者更极端的数据的概率有多大。

The *reason* the p-value is so commonly misinterpreted is simple – it’s more *natural* to study the uncertainty of a yet-unproven hypothesis (whether you have the rare disease) than the uncertainty of data we have already observed (you tested positive for the rare disease).

## Chapter2 Bayes Rule

**似然：已经发生的事件概率**

P(A|B) 表示已知事件 B 发生时 A 发生的条件概率，但是如果不知道事件 B 是否发生，但是知道 A 已经发生，此时我们可以比较 P(A|B) 和 $P(A|B^c)$ 来得知在不同事件 B 的状态下观测到数据 A 的 **相对似然：**

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/undefinedimage-20221017145705-8j4mu1j.png)

条件概率的引出：两个事件 A,B 的联合概率，也就是 A,B 同时发生的概率可以看成 B 的边缘概率乘以在 B 发生的条件下 A 发生的概率（逻辑，B 发生，然后在 B 发生的条件下 A 发生），因此可以得到 B 发生的条件下 A 发生的 **条件概率：**

$$
P(A|B)=\frac{P(A\cap B)}{P(B)}
$$

我们使用两次条件概率就可以得到贝叶斯定理：

$$
P(B |A) = \frac{P(A \cap B)}{P(A)} = \frac{P(B)L(B|A)}{P(A)}
$$

这里面的 L 就是似然函数，更一般的：

$$
\text{posterior} = \frac{\text{prior } \cdot \text{ likelihood}}{\text{normalizing constant}}
$$

这里的标准化的常数由全概率公式得到（**Law of Total Probability (LTP)**）

代码模拟：

```r
library(bayesrules)
library(tidyverse)
library(janitor)

# Define possible articles
article <- data.frame(type = c("real", "fake"))
# Define the prior model
prior <- c(0.6, 0.4)
set.seed(84735)
article_sim <- sample_n(article, size = 10000, 
                        weight = prior, replace = TRUE)
article_sim <- article_sim %>% 
  mutate(data_model = case_when(type == "fake" ~ 0.2667,
                                type == "real" ~ 0.0222))

glimpse(article_sim)
# Define whether there are exclamation points
data <- c("no", "yes")
# Simulate exclamation point usage 
set.seed(3)
article_sim <- article_sim %>%
  rowwise() %>% 
  mutate(usage = sample(data, size = 1, 
                        prob = c(1 - data_model, data_model)))
article_sim %>% 
  tabyl(usage, type) %>% 
  adorn_totals(c("col","row"))
# usage fake real Total
# no 2961 5833  8794
# yes 1070  136  1206
# Total 4031 5969 10000


article_sim %>% 
  filter(usage == "yes") %>% 
  tabyl(type) %>% 
  adorn_totals("row")
# type    n   percent
# fake 1070 0.8872305
# real  136 0.1127695
# Total 1206 1.0000000
```

### 随机变量的贝叶斯模型

离散概率模型：随机变量有有限个取值，每个取值对应一个概率值，这个对应关系由**概率密度函数**得到（**probability mass function (pmf)**）。

**第一步**根据已有的知识得到一个先验的概率模型，例子：1996 年 Kasparov 和深蓝对战，赢了 3局，平了两局，输了一局，因此构建一个先验模型（假设赢的几率只有三种取值），那么这个先验模型中反映的是 Kasparov 有较大的概率赢下比赛：

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/undefinedimage-20221206163808-w6ol076.png)

‍

**第二步**就是收集处理数据，从而提升对随机变量的理解。Y 是随机变量，表示在 1997 年的比赛中 Kasparov 赢的局数，可以为 1~6 的数字，因此我们需要对 **Y 与 π 的依赖关系建模**，也就是建立一个条件概率模型：

$$
f(y|\pi) = P(Y = y | \pi)
$$

需要做两个假设：

1. 每局游戏之间是独立的，一局的结果并不会影响另一局
2. 赢每一局的概率都是相等的，π

因此这种情况可以使用二项式模型来建模：

$$
Y | \pi \sim \text{Bin}(n,\pi)
$$

在这里就是：

$$
Y | \pi \sim \text{Bin}(6,\pi)
$$

$$
\begin{equation}
f(y|\pi) = \left(\!\begin{array}{c} 6 \\ y \end{array}\!\right) \pi^y (1 - \pi)^{6 - y} \;\; \text{ for } y \in \{0,1,2,3,4,5,6\}  .
\tag{2.8}
\end{equation}
$$

在实际比赛中 Kasparov 只赢了一局，根据这个数据我们可以来更新不同获胜概率的信念，也就是计算似然函数：

$$
L(\pi | y = 1) = f(y=1 | \pi) = \left(\!\begin{array}{c} 6 \\ 1 \end{array}\!\right) \pi^1 (1-\pi)^{6-1} = 6\pi(1-\pi)^5  .
$$

那么这个就是二项式似然函数，可以计算不同获胜概率时的似然函数取值：

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/undefinedimage-20230306221504-n2l0s53.png)

> $L(\cdot | y)$**​**​ ​**provides the tool we need to evaluate the relative **compatibility ​**of data **Y**=**y ​**with various π values.

**第三步**就是计算我们的常数来平衡先验和似然，使用全概率公式：

$$
f(y = 1) = \sum_{\pi \in \{0.2,0.5,0.8\}} L(\pi | y=1) f(\pi)
$$

$$
\begin{equation}
\begin{split}
f(y = 1) 
& = L(\pi = 0.2 | y=1) f(\pi = 0.2) + L(\pi = 0.5 | y=1) f(\pi = 0.5) \\
& \hspace{.2in} + L(\pi = 0.8 | y=1) f(\pi = 0.8) \\
& \approx 0.3932 \cdot 0.10 + 0.0938 \cdot 0.25 + 0.0015 \cdot 0.65 \\
& \approx 0.0637  . \\
\end{split}
\tag{2.9}
\end{equation}
$$

总结：

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/undefinedimage-20230306222733-hmziel2.png)

左边的图是先验，中间的图是似然，右边的图是后验，也就是先验在观测到数据后发生了变化，成了最右边的后验分布。

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/undefinedimage-20230306222823-busv3ht.png)

## Chapter3 The Beta-Binomial Bayesian Model

问题引入：Michelle 要参加大选，收集了 30 次民意调查的结果，以此构建先验模型，注意这里的先验模型不再是上一章中的离散模型，而是连续的模型，其支持率（$\pi$）在 0-1 之间：

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/undefinedimage-20230323083908-q3ttprg.png)

> 连续随机变量的概率密度函数可以大于一，在某个随机变量的取值下的概率密度函数值可以大于1，因此表示的一个相对的概念

### Beta 模型介绍 -- 先验模型

如果 $\pi$ 是 0-1 上的随机变量，那么 $\pi$ 的不确定性可以由 beta 模型来建模，超参数是 $\alpha$ 和 $\beta$：

$$
\pi \sim \text{Beta}(\alpha, \beta).
$$

其概率密度函数为：

$$
f(\pi) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \pi^{\alpha-1} (1-\pi)^{\beta-1} \;\; \text{ for } \pi \in [0,1] \tag{3.1}
$$

$$
\Gamma(z) = \int_0^\infty x^{z-1}e^{-y}dx
$$

不同的 $\alpha$ 和 $\beta$ 下，概率密度函数的形状也不同：

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/undefinedimage-20230323090022-iwetng8.png)

beta 模型的一些性质：

* $$
  \begin{equation}
  \begin{split}
  E(\pi) & = \frac{\alpha}{\alpha + \beta} \\
  \text{Mode}(\pi) & = \frac{\alpha - 1}{\alpha + \beta - 2} \;\;\; \text{ when } \; \alpha, \beta > 1. \\
  \end{split}
  \tag{3.2}
  \end{equation}
  $$

* $$
  \begin{equation}
  \text{Var}(\pi) = \frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta + 1)} .
  \tag{3.3}
  \end{equation}
  $$

现在我们可以为之前的民意调查选择一个先验的模型了（通过调节 $\alpha$ 和 $\beta$），通过观察发现，均值大概在 0.45 左右，也就是：

‍

$$
\frac{\alpha}{\alpha+ \beta}\approx0.45
$$

可以尝试 Beta(9,11), Beta(27,33), Beta(45,55)，发现 45 和 55 比较符合：

```r
plot_beta(45, 55)
```

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/undefinedimage-20230324103110-7t8szk7.png)

### 二项式数据以及似然函数 -- 收集数据

贝叶斯分析的第二个步骤就是收集一些新的数据，这里做了 50 次新的民意调查，支持人数为随机变量 Y，假设：

* 每次投票是独立的
* 每次投票支持 Michelle 的概率都是 $\pi$​

因此随机变量 Y 服从二项分布：

$$
Y | \pi \sim \text{Bin}(50, \pi)
$$

我们可以得到 Y 关于 $\pi$ 的似然函数：

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/undefinedimage-20230324111010-4quts4d.png)

### Beta 后验模型 -- 更新

基于我们的先验模型和收集的数据（50 次里面有 30 次支持），我们可以得到后验模型：

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/undefinedimage-20230324140231-ohl271i.png)

根据贝叶斯定理可以得到：

$$
f(\pi | y = 30) = \frac{f(\pi)L(\pi|y = 30)}{f(y = 30)}.
$$

分母是一个标准化的因子，因此可以不纳入计算，然后代入先验的 beta 分布和数据的二项分布：

$$
\begin{split}
f(\pi | y = 30)
& \propto f(\pi) L(\pi | y=30)  \\
& = \frac{\Gamma(100)}{\Gamma(45)\Gamma(55)}\pi^{44}(1-\pi)^{54} \cdot \left(\!\!\begin{array}{c} 50 \\ 30 \end{array}\!\!\right) \pi^{30} (1-\pi)^{20}  \\
& = \left[\frac{\Gamma(100)}{\Gamma(45)\Gamma(55)}\left(\!\!\begin{array}{c} 50 \\ 30 \end{array}\!\!\right) \right] \cdot \pi^{74} (1-\pi)^{74}  \\
& \propto \pi^{74} (1-\pi)^{74}  . \\
\end{split}
$$

这里面的 $\pi^{74} (1-\pi)^{74}$ 叫做概率密度函数的 **kernel ，**并且这个和 Beta(75,75) 的 kernel 是一样的：

$$
Beta(75,75) = \frac{\Gamma(150)}{\Gamma(75)\Gamma(75)} \pi^{74} (1-\pi)^{74} \propto \pi^{74} (1-\pi)^{74}  .
$$

因此后验分布服从 $\alpha$ = 75，$\beta$ = 75 的 Beta 分布。**这种后验分布和先验分布是一个分布家族的**，我们称这个 Beta 模型是相应的二项模型的共轭先验（**Conjugate prior**）。

> We say that **f**(**π)** is a conjugate prior for **L**(**π**|**y**) if the posterior, **f**(**π**|**y**)**∝**f**(**π**)**L**(**π**|**y**)**, is from the same model family as the prior.

### Beta-Binomial 模型

上面的是一种最基础的 Beta-Binomial 模型，由先验的 beta 模型和数据的 Binomial 模型构成；一般的 Beta-Binomial ：

$$
\begin{split}
Y | \pi & \sim \text{Bin}(n, \pi) \\
\pi & \sim \text{Beta}(\alpha, \beta). \\
\end{split}
$$

在 n 次实验中观测到 Y=y 次成功的数据后，$\pi$ 的后验可以由一个 Beta 模型来描述，反映了先验（参数是 $\alpha$ 和 $\beta$）和数据的影响：

$$
\begin{equation}
\pi | (Y = y) \sim \text{Beta}(\alpha + y, \beta + n - y)  .
\tag{3.10}
\end{equation}
$$

得到这个的过程和上面类似：

$$
\begin{equation}
f(\pi) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\pi^{\alpha - 1}(1-\pi)^{\beta - 1} 
\;\; \text{ and } \;\; 
L(\pi|y) = \left(\!\!\begin{array}{c} n \\ y \end{array}\!\!\right) \pi^{y} (1-\pi)^{n-y}  .
\tag{3.12}
\end{equation}
$$

$$
\begin{split}
f(\pi | y)
& \propto f(\pi)L(\pi|y) \\
& = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\pi^{\alpha - 1}(1-\pi)^{\beta - 1}  \cdot \left(\!\begin{array}{c} n \\ y \end{array}\!\right) \pi^{y} (1-\pi)^{n-y} \\
& \propto \pi^{(\alpha + y) - 1} (1-\pi)^{(\beta + n - y) - 1}  .\\
\end{split}
$$

我们可以来做一些模拟：使用 `rbeta()`​ 从 Beta 分布随机抽取 10000 个 $\pi$ 值，然后对每个 $\pi$ 使用 `rbinom()`​从二项分布随机抽取 10000 个 Y：

```r
set.seed(84735)
michelle_sim <- data.frame(pi = rbeta(10000, 45, 55)) %>% 
  mutate(y = rbinom(10000, size = 50, prob = pi))

ggplot(michelle_sim, aes(x = pi, y = y)) + 
  geom_point(aes(color = (y == 30)), size = 0.1)
```

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/undefinedimage-20230324143646-6qmi239.png)

然后对于 Y = 30，我们可以展示在不同的 $\pi$ 下的密度：

```r
# Keep only the simulated pairs that match our data
michelle_posterior <- michelle_sim %>% 
  filter(y == 30)

# Plot the remaining pi values
ggplot(michelle_posterior, aes(x = pi)) + 
  geom_density()
```

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/undefinedimage-20230324143813-8dv5dtz.png)

可以看到这个和后验的 Beta(75,75) 分布是很接近的。

总结：这一章主要讲了 Beta-Binomial 模型：

$$
\begin{split}
Y | \pi & \sim \text{Bin}(n, \pi) \\
\pi & \sim \text{Beta}(\alpha, \beta) \\
\end{split} \;\; \Rightarrow \;\; 
\pi | (Y = y) \sim \text{Beta}(\alpha + y, \beta + n - y)  .
$$

这个模型反映了贝叶斯数据分析的一般的四个步骤：

1. 先验模型，通过调节 Beta 模型的 $\alpha$ 和 $\beta$ 的超参数来反映 $\pi$ 的可能性
2. 数据模型，n 次独立实验中成功的次数，服从二项分布
3. 似然函数，依据实际观测到的数据，代入数据模型，得到似然函数（二项分布概率密度函数），反映了不同 $\pi$ 和数据的相容性
4. 后验模型，通过贝叶斯定律，结合共轭的 Beta 先验和二项数据分布，得到后验的 Beta 模型

## Chapter 4 贝叶斯分析中的平衡和顺序性

问题引入：电影中的女性形象的 Bechdel test，如果一个电影符合下面三个标准即通过该检验：

* 至少有两个女性角色
* 两个女性角色之间相互交谈
* 谈论的内容和男性无关

设 $\pi$ 为通过检验的电影占比，有三个人分别代表着女性主义者，中立主义者和乐观主义者，对这个 $\pi$ 有着各种的先验观念，我们可以使用 β 模型（通过调整不同的 α 和 β 参数）来对他们的先验观念进行建模：

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/undefinedch4-bechdel-priors-1-20230702163610-m8xsime.png)

先验分布的变异性（variability）反应了对先验信息的确定程度，越不确定，变异程度就越大，比如中立主义者的均匀分布，这类先验称为模糊先验（**Vague prior**）；而乐观主义者则对其先验信息非常确定，我们将这类先验称为有信息的先验（**Informative prior**）。

### 不同的先验--不同的后验

现在收集一些新数据：记录最近的 n 个电影中通过 Bechdel test 的个数 Y，因此这个数据服从参数为 $\pi$ 的二项分布，参数为 α 和 β 的 Beta 二项分布：

$$
\begin{split}
Y | \pi & \sim \text{Bin}(n, \pi)  \\
\pi & \sim \text{Beta}(\alpha, \beta) \\ 
\end{split}
$$

因此可以得到独特的 π 后验模型，取决于独特的先验分布（通过α和β）和共同的观察数据（通过y和n）：

$$
\pi | (Y = y) \sim \text{Beta}(\alpha + y, \beta + n - y)
$$

```R
##导入数据
data(bechdel, package = "bayesrules")

#随机抽样20个电影
set.seed(84735)
bechdel_20 <- bechdel %>% 
  sample_n(20)

bechdel_20 %>% 
  head(3)
##binary 变量表示该电影是否通过测试
# A tibble: 3 x 3
   year title      binary
  <dbl> <chr>      <chr> 
1  2005 King Kong  FAIL  
2  1983 Flashdance PASS  
3  2013 The Purge  FAIL  

bechdel_20 %>% 
  tabyl(binary) %>% 
  adorn_totals("row")
 binary  n percent
   FAIL 11    0.55
   PASS  9    0.45
  Total 20    1.00
###9 个电影通过了测验
```

可以根据上面的式子算出各自的后验分布：

|Analyst|Prior|Posterior|
| ----------| ------------| -------------|
|女性主义|Beta(5,11)|Beta(14,22)|
|中立主义|Beta(1,1)|Beta(10,12)|
|乐观主义|Beta(14,1)|Beta(23,12)|

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/undefinedbechdel-post-ch4-1-20230702165613-2yb2ymj.png)

可以看到对于中立主义者来说其先验分布没有任何信息，因此其后验分布和似然是一致的。

### 不同的数据--不同的后验

现在来实验下不同的数据对后验的影响，假设先验分布都是上面的乐观主义者的先验，但是收集的数据不同：

```R
bechdel %>% 
  filter(year == 1991) %>% 
  tabyl(binary) %>% 
  adorn_totals("row")
 binary  n percent
   FAIL  7  0.5385
   PASS  6  0.4615
  Total 13  1.0000

bechdel %>% 
  filter(year == 2000) %>% 
  tabyl(binary) %>% 
  adorn_totals("row")
 binary  n percent
   FAIL 34  0.5397
   PASS 29  0.4603
  Total 63  1.0000

bechdel %>% 
  filter(year == 2013) %>% 
  tabyl(binary) %>% 
  adorn_totals("row")
 binary  n percent
   FAIL 53  0.5354
   PASS 46  0.4646
  Total 99  1.0000
```

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/undefinedbechdel-data-ch4-1-20230702171041-3j50vl0.png)

|数据|后验|
| --------| -------------|
|**Y**=**6** of **n**=**13**|Beta(20,8)|
|**Y**=**29** of **n**=**63**|Beta(43,35)|
|**Y**=**46** of **n**=**99**|Beta(60,54)|

可以看到数据量越大（虽然这些数据中通过检验的比例是相同的，46%）后验分布就越接近似然函数，数据对后验的影响也就越大。

### 在先验和数据之间平衡

数据和先验对后验的影响可以用下图来总结：

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/undefinedbechdel-combined-ch4-1-20230702172233-67e3f3u.png)

* 数据量越大，后验分布越接近似然函数，先验分布对后验分布的影响越小（从左到右）
* 先验含有的信息越多，对后验的影响越大（从上到下）
* 注意最后一列，当数据量足够大时，不管什么先验分布都可以得到相同的后验分布

> 但是我们需要注意，当先验选择的不恰当时也会出现无论多大的数据也无法消除先验影响的情况。比如先验分布为 :
>
> $$
> \pi \sim \text{Unif}(0,0.25)
> $$
>
> $$
> f(\pi) = 4 \; \text{ for } \pi \in [0, 0.25]
> $$
>
> 后验分布：
>
> $$
> \begin{split}
> f(\pi | y=8) 
> & \propto f(\pi)L(\pi | y=8) \\
> & = 4 \cdot \left(\!\begin{array}{c} 10 \\ 8 \end{array}\!\right) \ \pi^{8} (1-\pi)^{2} \\
> & \propto \pi^{8} (1-\pi)^{2}. \\
> \end{split}
> $$
>
> ![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/undefinedimage-20230702211858-gqzc0ho.png)

这个结论我们可以通过推导得到：

对于上面的后验模型，我们可以计算其 beta 分布的均值：

$$
E(\pi|Y=y)=\frac{\alpha+y}{\alpha + \beta +n}
$$

可以将这个式子重写成先验和数据的组合：

$$
\begin{split}
E(\pi | Y=y)  
& = \frac{\alpha}{\alpha + \beta + n} + \frac{y}{\alpha + \beta + n}  \\
& = \frac{\alpha}{\alpha + \beta + n}\cdot\frac{\alpha + \beta}{\alpha + \beta} + \frac{y}{\alpha + \beta + n}\cdot\frac{n}{n}  \\
& = \frac{\alpha + \beta}{\alpha + \beta + n}\cdot\frac{\alpha}{\alpha + \beta} + \frac{n}{\alpha + \beta + n}\cdot\frac{y}{n}  \\
& = \frac{\alpha + \beta}{\alpha + \beta + n}\cdot E(\pi) + \frac{n}{\alpha + \beta + n}\cdot\frac{y}{n}  .  \\
\end{split}
$$

因此后验的均值可以看成是先验均值和数据（样本）成功率的**加权平均，**这个权重的和也是 1；我们可以以上面表的第一行和最后一行为例：

$$
\begin{split}
E(\pi | Y=6)  
& = \frac{14 + 1}{14 + 1 + 13} \cdot E(\pi) + \frac{13}{14 + 1 + 13}\cdot\frac{y}{n} \\
& = 0.5357 \cdot \frac{14}{15} + 0.4643 \cdot \frac{6}{13}   \\
& = 0.7143  .  \\
\end{split}
$$

$$
\begin{split}
E(\pi | Y=46)  
& = \frac{14 + 1}{14 + 1 + 99} \cdot E(\pi) + \frac{99}{14 + 1 + 99}\cdot\frac{y}{n} \\
& = 0.1316 \cdot \frac{14}{15} + 0.8684 \cdot \frac{46}{99}   \\
& = 0.5263  .  \\
\end{split}
$$

尽管先验的均值是一样的（14/15），样本的成功率也差不多（46%），但是由于数据量的不同造成了先验和数据的权重的差别；实际上当 n 趋向于无穷大的时候，先验均值的权重趋向于 0，而数据的权重趋向于 1。因此当收集的数据越来越多，后验模型会越来越倾向于数据而不是先验。这个越来越多就反映了序贯贝叶斯分析或者叫做贝叶斯学习：

> In a sequential Bayesian analysis, a posterior model is updated  incrementally as more data come in. With each new piece of data, the  previous posterior model reflecting our understanding prior to observing  this data becomes the new prior model.

序贯分析有两个基本性质：

1. 最终得到的结果和数据的收集次序无关
2. 最终的后验结果只依赖于累积的数据（一步收集也行）

设 $\theta$ 是先验分布的参数，两个时间点分别收集的数据为 $y_1$ 和 $y_2$ ，那么：

$$
f(\theta | y_1,y_2) = f(\theta|y_2,y_1)
$$

‍
