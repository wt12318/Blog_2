

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/DNA.png">
  <link rel="icon" href="/img/DNA.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="wutao">
  <meta name="keywords" content="">
  
  <title>深度学习入门 - Hexo</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.6.0/styles/atelier-sulphurpool-dark.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.10","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"A5lchRnum84Yumu5pVOSoVN8-MdYXbMMI","app_key":"runF3Mxw7648v64CISxiqMs8","server_url":null}}};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>wutao's blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="深度学习入门">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-01-03 10:00" pubdate>
        2021年1月3日 上午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      4.7k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      62
       分钟
    </span>
  

  
  
    
      <!-- LeanCloud 统计文章PV -->
      <span id="leancloud-page-views-container" class="post-meta" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="leancloud-page-views"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">深度学习入门</h1>
            
              <p class="note note-info">
                
                  本文最后更新于：5 个月前
                
              </p>
            
            <div class="markdown-body">
              <p>主要内容来自<em>深度学习入门</em>：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20201227171954771.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><p>感知机也可以称为人工神经元，是神经网络的基础</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20201223224321725.png" srcset="/img/loading.gif" lazyload></p>
<p>感知机接受多个信号(x1,x2)，输出一个信号(y), w1/w2是权重，圆圈就代表神经元</p>
<p>输入信号被传递到神经元的时候会被乘上<strong>权重</strong>(<em>w</em>1<em>x</em>1、<em>w</em>2<em>x</em>2)，神经元会计算输入的信号总和，只有这个总和超过某个阈值才会输出1，这个状态就叫做神经元的激活，这个过程用函数表示如下( $\theta$表示阈值)：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20201223224937885.png" srcset="/img/loading.gif" lazyload></p>
<p>也可以对上式进行形式上的转化(将$\theta$移到左边)：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20201223225500139.png" srcset="/img/loading.gif" lazyload></p>
<p>这里面b叫做<strong>偏置</strong></p>
<p>可以看出<em>w</em>1和<em>w</em>2是控制输入信号的重要性的参数，而偏置b是调整神经元被激活的容易程度的参数</p>
<p>我们再进一步简化上面函数的形式，引入一个新的函数$h(x)$，将上面的函数改写：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20201223230119601.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20201223230140674.png" srcset="/img/loading.gif" lazyload></p>
<p>函数$h(x)$对输入信号的总和进行转化，转化后的值就是输出y</p>
<p>这个函数$h(x)$就是<strong>激活函数</strong></p>
<p>我们可以将上面的感知机进行细化，展示出激活函数的运算过程：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20201223230609708.png" srcset="/img/loading.gif" lazyload></p>
<p>$a=w_1x_1+w_2x_2+b$, $y=h(a)$ </p>
<p><em>a</em>计算计算加权输入信号和偏置的总和，用<em>h</em>(x)函数将<em>a</em>转换为输出<em>y</em></p>
<p>这样的激活函数称为<strong>阶跃函数</strong>，超过某个阈值就会改变函数的输出，函数的图像呈阶梯状：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20201223231248638.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>神经网络可以看作是多层感知机，并且使用的激活函数不再是阶跃函数了</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20201223233340355.png" srcset="/img/loading.gif" lazyload></p>
<p>上图展示的是一个简单的3层(从0开始计算)神经网络</p>
<p>先来看一下常用的激活函数：</p>
<h3 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a><code>sigmoid</code>函数</h3><p><code>sigmoid</code>函数的表示为：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20201223233716730.png" srcset="/img/loading.gif" lazyload></p>
<p>可以通过<code>python</code>简单的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pylab <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment">###sigmoid function</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x)) <br><br>X = np.arange(-<span class="hljs-number">5.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">0.1</span>)<br>Y = sigmoid(X)<br>plt.plot(X, Y)<br>plt.ylim(-<span class="hljs-number">0.1</span>, <span class="hljs-number">1.1</span>)<br>plt.show()<br></code></pre></div></td></tr></table></figure>

<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20201223234952100.png" srcset="/img/loading.gif" lazyload></p>
<p><code>sigmoid</code>函数和阶跃函数的主要区别在于其是平滑的曲线，连续可导</p>
<h3 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a><code>ReLU</code>函数</h3><p><code>ReLU</code>全称为Rectified Linear Unit，线性整流函数</p>
<p><code>ReLU</code>函数在输入大于0时，直接输出该值；在输入小于等于0时，输出0:</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103110740713.png" srcset="/img/loading.gif" lazyload alt="image-20210103110740713"></p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment">##relu</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">relu</span>(<span class="hljs-params">x</span>):</span><br>    <span class="hljs-keyword">return</span> np.maximum(<span class="hljs-number">0</span>, x)<br><br>x = np.arange(-<span class="hljs-number">5.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">0.1</span>)<br>y = relu(x)<br>plt.plot(x, y)<br>plt.ylim(-<span class="hljs-number">1.0</span>, <span class="hljs-number">5.5</span>)<br>plt.show()<br></code></pre></div></td></tr></table></figure>

<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103110758248.png" srcset="/img/loading.gif" lazyload alt="image-20210103110758248"></p>
<h3 id="3层神经网络的实现"><a href="#3层神经网络的实现" class="headerlink" title="3层神经网络的实现"></a>3层神经网络的实现</h3><p>各层间信号的传递可以用下图来表示：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103110840483.png" srcset="/img/loading.gif" lazyload alt="image-20210103110840483"></p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment">#####three layer network</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init_network</span>():</span><br> network = &#123;&#125;<br> network[<span class="hljs-string">&#x27;W1&#x27;</span>] = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.5</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.6</span>]])<br> network[<span class="hljs-string">&#x27;b1&#x27;</span>] = np.array([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>])<br> network[<span class="hljs-string">&#x27;W2&#x27;</span>] = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.4</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.5</span>], [<span class="hljs-number">0.3</span>, <span class="hljs-number">0.6</span>]])<br> network[<span class="hljs-string">&#x27;b2&#x27;</span>] = np.array([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>])<br> network[<span class="hljs-string">&#x27;W3&#x27;</span>] = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.4</span>]])<br> network[<span class="hljs-string">&#x27;b3&#x27;</span>] = np.array([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>])<br> <span class="hljs-keyword">return</span> network<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">network, x</span>):</span><br> W1, W2, W3 = network[<span class="hljs-string">&#x27;W1&#x27;</span>], network[<span class="hljs-string">&#x27;W2&#x27;</span>], network[<span class="hljs-string">&#x27;W3&#x27;</span>]<br> b1, b2, b3 = network[<span class="hljs-string">&#x27;b1&#x27;</span>], network[<span class="hljs-string">&#x27;b2&#x27;</span>], network[<span class="hljs-string">&#x27;b3&#x27;</span>]<br> a1 = np.dot(x, W1) + b1<br> z1 = sigmoid(a1)<br> a2 = np.dot(z1, W2) + b2<br> z2 = sigmoid(a2)<br> a3 = np.dot(z2, W3) + b3<br> y = identity_function(a3)<br> <span class="hljs-keyword">return</span> y<br><br><br>network = init_network()<br>x = np.array([<span class="hljs-number">1.0</span>, <span class="hljs-number">0.5</span>])<span class="hljs-comment">##初始输入</span><br>y = forward(network, x)<br><span class="hljs-built_in">print</span>(y)<br></code></pre></div></td></tr></table></figure>

<p>这里面需要注意的点是输出层的激活函数使用的是恒等函数，一般地，回归问题可以使用恒等函数，二元分类问题可以使用 sigmoid函数，多元分类问题可以使用 softmax函数</p>
<p>softmax函数可以表示为：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20201227183549421.png" srcset="/img/loading.gif" lazyload></p>
<p>但是这个函数在进行计算的时候，指数运算可能会产生非常大的数(可能会出现Inf)，需要对其进行变换：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103110919951.png" srcset="/img/loading.gif" lazyload alt="image-20210103110919951"></p>
<p>可以将$C’$替换为输入信号的最大值的负值来减小运算：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">softmax</span>(<span class="hljs-params">a</span>):</span><br> c = np.<span class="hljs-built_in">max</span>(a)<br> exp_a = np.exp(a - c) <span class="hljs-comment"># 溢出对策</span><br> sum_exp_a = np.<span class="hljs-built_in">sum</span>(exp_a)<br> y = exp_a / sum_exp_a<br> <span class="hljs-keyword">return</span> y<br><br> a = np.array([<span class="hljs-number">0.3</span>, <span class="hljs-number">2.9</span>, <span class="hljs-number">4.0</span>])<br> y = softmax(a)<br> <span class="hljs-built_in">print</span>(y)<span class="hljs-comment">#[0.01821127 0.24519181 0.73659691]</span><br> np.<span class="hljs-built_in">sum</span>(y)<span class="hljs-comment">#1.0</span><br></code></pre></div></td></tr></table></figure>

<p>softmax函数会将输入信号映射到0~1上，并且转化后的值总和为1，这就使我们可以把softmax函数的输出解释为</p>
<p>概率值；另外指数函数在求导时也比较方便(导数还是指数函数)</p>
<h2 id="神经网络的学习"><a href="#神经网络的学习" class="headerlink" title="神经网络的学习"></a>神经网络的学习</h2><p>神经网络的学习过程就是通过某种指标来寻找最优权重参数，这个指标就称为<strong>损失函数</strong>,损失函数表示的是神经网络对当前训练数据在多大程度上不拟合，所以目标就是使损失函数尽可能小</p>
<p>损失函数一般使用均方误差和交叉熵误差</p>
<p><strong>均方误差</strong>如下所示：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103110936719.png" srcset="/img/loading.gif" lazyload alt="image-20210103110936719"></p>
<p>y表示神经网络得输出，t表示训练数据，k表示维度</p>
<p>分类问题和回归问题都可以使用均方误差</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment">###mean_squared_error</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mean_squared_error</span>(<span class="hljs-params">y, t</span>):</span><br> <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span> * np.<span class="hljs-built_in">sum</span>((y-t)**<span class="hljs-number">2</span>)<br><br><br>t = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]<br>y = [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.05</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.05</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>]<br>mean_squared_error(np.array(y), np.array(t))<br><span class="hljs-comment">##0.09750000000000003</span><br><br>y = [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.05</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.05</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>]<br>mean_squared_error(np.array(y), np.array(t))<br><span class="hljs-comment">##0.5975</span><br></code></pre></div></td></tr></table></figure>

<p><strong>交叉熵误差</strong>一般作为分类问题的损失函数：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103110944944.png" srcset="/img/loading.gif" lazyload alt="image-20210103110944944"></p>
<p>y表示神经网络的输出，t表示训练数据的标签，相应的类标签为1，其他为0，所以交叉熵误差只计算正确标签对应的神经网络的输出，当这个输出越接近于1，E值就越小(等于1时,E=0)</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cross_entropy_error</span>(<span class="hljs-params">y, t</span>):</span><br> delta = <span class="hljs-number">1e-7</span><br> <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(t * np.log(y + delta))<br><br>t = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]<br>y = [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.05</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.05</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>]<br>cross_entropy_error(np.array(y), np.array(t))<br><span class="hljs-comment">##0.510825457099338</span><br><br>y = [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.05</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.05</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>]<br>cross_entropy_error(np.array(y), np.array(t))<br><span class="hljs-comment">##2.302584092994546</span><br></code></pre></div></td></tr></table></figure>

<p>神经网络学习的任务就是找到损失函数取最小值(或尽可能小)时的参数(权重和偏置)，这个过程可以通过梯度法来实现</p>
<h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>在介绍梯度之前需要知道导数的概念</p>
<p>导数表示的是某个瞬间的变化量，即<em>x</em>的微小变化将导致函数<em>f</em>（<em>x</em>）的值在多大程度上发生变化：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20201224122322725.png" srcset="/img/loading.gif" lazyload></p>
<p>在实现求导的时候，可以人为导入一个微小的h，并且使用中心差分的形式($f(x+h)-f(x-h)$,以x为中心可以减少误差)</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">numerical_diff</span>(<span class="hljs-params">f, x</span>):</span><br> h = <span class="hljs-number">1e-4</span><br> <span class="hljs-keyword">return</span> (f(x+h) - f(x-h)) / (<span class="hljs-number">2</span>*h)<br></code></pre></div></td></tr></table></figure>

<p>这种通过微小的差分来求导数的过程称为数值微分，和利用数学推导求导数的解析性求导区分</p>
<p>当我们需要对有多个变量的函数中每个变量进行求导，这个时候的导数就叫做偏导数，求偏导数就是将某个变量定为目标变量，其余变量固定为常数，然后对目标变量求导的过程</p>
<p>比如有一个二变量的函数：$f(x_0,x_1)=x_0^2+x_1^2$ ,求$x_0=3,x_1=4$的时候，关于各个变量的偏导数</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment">##定义函数</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">function_2</span>(<span class="hljs-params">x</span>):</span><br>	<span class="hljs-keyword">return</span> x[<span class="hljs-number">0</span>]**<span class="hljs-number">2</span> + x[<span class="hljs-number">1</span>]**<span class="hljs-number">2</span><br></code></pre></div></td></tr></table></figure>

<p>求偏导就是将无关的变量设为常量：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment">##x0</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">function_tmp1</span>(<span class="hljs-params">x0</span>):</span><br>	<span class="hljs-keyword">return</span> x0*x0 + <span class="hljs-number">4.0</span>**<span class="hljs-number">2.0</span><br><br>numerical_diff(function_tmp1, <span class="hljs-number">3.0</span>)<span class="hljs-comment">#6.00000000000378</span><br><br><span class="hljs-comment">##x1</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">function_tmp2</span>(<span class="hljs-params">x1</span>):</span><br>	<span class="hljs-keyword">return</span> <span class="hljs-number">3.0</span>**<span class="hljs-number">2.0</span> + x1*x1<br><br>numerical_diff(function_tmp2, <span class="hljs-number">4.0</span>)<span class="hljs-comment">#7.999999999999119</span><br></code></pre></div></td></tr></table></figure>

<p>由全部变量的偏导数汇总而成的向量称为<strong>梯度</strong>（gradient）比如对于上面的二变量函数，梯度就是$(\frac{\partial f}{\partial x_0},\frac{\partial f}{\partial x_1})$ 构成的向量：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">numerical_gradient</span>(<span class="hljs-params">f, x</span>):</span><br>    h = <span class="hljs-number">1e-4</span> <br>    grad = np.zeros_like(x) <span class="hljs-comment"># 生成和x形状相同的数组</span><br>    <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(x.size):<br>        tmp_val = x[idx]<br>        <span class="hljs-comment"># f(x+h)的计算</span><br>        x[idx] = tmp_val + h<br>        fxh1 = f(x)<br>        <span class="hljs-comment"># f(x-h)的计算</span><br>        x[idx] = tmp_val - h<br>        fxh2 = f(x)<br>        grad[idx] = (fxh1 - fxh2) / (<span class="hljs-number">2</span>*h)<br>        x[idx] = tmp_val <span class="hljs-comment"># 还原值</span><br>    <span class="hljs-keyword">return</span> grad<br><br>numerical_gradient(function_2, np.array([<span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>]))<br><span class="hljs-comment">#array([6., 8.])</span><br></code></pre></div></td></tr></table></figure>

<p>计算这个函数各点的梯度：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111038666.png" srcset="/img/loading.gif" lazyload alt="image-20210103111038666"></p>
<p>这个箭头就代表了梯度，可以看到梯度的方向指向这个函数的最小值(0,0); 虽然在其他情况下梯度的方向并不总是指向最小值，但沿着它的方向能够最大限度地减小函数的值；因此通过不断地沿梯度方向前进，逐渐减小函数值的过程就是<strong>梯度法</strong>（gradient method）(一般指的的梯度下降)</p>
<p>用数学式表示梯度法：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111049842.png" srcset="/img/loading.gif" lazyload alt="image-20210103111049842"></p>
<p>其中<em>η</em>表示更新量，表示每次沿着梯度的方向下降的程度，在神经网络中称为<strong>学习率</strong>(learning rate)</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment">##gradient_descent</span><br><span class="hljs-comment">##f 进行最优化的函数</span><br><span class="hljs-comment">##init_x 初始值</span><br><span class="hljs-comment">##lr 学习率</span><br><span class="hljs-comment">##step_num 迭代次数</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gradient_descent</span>(<span class="hljs-params">f, init_x, lr=<span class="hljs-number">0.01</span>, step_num=<span class="hljs-number">100</span></span>):</span><br>    x = init_x<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(step_num):<br> 	    grad = numerical_gradient(f, x)<br> 	    x -= lr * grad<br>    <span class="hljs-keyword">return</span> x<br><br>init_x = np.array([-<span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>])<br>gradient_descent(function_2, init_x=init_x, lr=<span class="hljs-number">0.1</span>, step_num=<span class="hljs-number">100</span>)<br><span class="hljs-comment">#array([-6.11110793e-10,  8.14814391e-10])</span><br></code></pre></div></td></tr></table></figure>

<p>像学习率这样的参数称为<strong>超参数</strong> ， 神经网络的权重参数是通过学习得到的，而超参数是人为设定的，所以需要尝试</p>
<h3 id="学习算法的实现"><a href="#学习算法的实现" class="headerlink" title="学习算法的实现"></a>学习算法的实现</h3><p>神经网络的学习步骤可以分成以下几步：</p>
<ul>
<li>mini-batch: 从训练数据中随机选出一部分数据，这部分数据称为mini batch。我们的目标是减小mini batch的损失函数的值</li>
<li>计算梯度：为了减小mini batch的损失函数的值，需要求出各个权重参数的梯度；梯度表示损失函数的值减小最多的方向</li>
<li>更新参数：将权重参数沿梯度方向进行微小更新</li>
<li>迭代：重复前3个步骤</li>
</ul>
<p>由于选择mini batch是随机的，因此这种方法叫做<strong>随机梯度下降法</strong>(stochastic gradient descent,SGD)</p>
<p>下面是两层神经网络的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment">###TwoLayerNet</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TwoLayerNet</span>:</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, output_size, weight_init_std=<span class="hljs-number">0.01</span></span>):</span><br>        <span class="hljs-comment"># 初始化权重</span><br>        self.params = &#123;&#125;<br>        self.params[<span class="hljs-string">&#x27;W1&#x27;</span>] = weight_init_std * np.random.randn(input_size, hidden_size)<br>        self.params[<span class="hljs-string">&#x27;b1&#x27;</span>] = np.zeros(hidden_size)<br>        self.params[<span class="hljs-string">&#x27;W2&#x27;</span>] = weight_init_std * np.random.randn(hidden_size, output_size)<br>        self.params[<span class="hljs-string">&#x27;b2&#x27;</span>] = np.zeros(output_size)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span>(<span class="hljs-params">self, x</span>):</span><br>        W1, W2 = self.params[<span class="hljs-string">&#x27;W1&#x27;</span>], self.params[<span class="hljs-string">&#x27;W2&#x27;</span>]<br>        b1, b2 = self.params[<span class="hljs-string">&#x27;b1&#x27;</span>], self.params[<span class="hljs-string">&#x27;b2&#x27;</span>]<br>    <br>        a1 = np.dot(x, W1) + b1<br>        z1 = sigmoid(a1)<br>        a2 = np.dot(z1, W2) + b2<br>        y = softmax(a2)<br>        <br>        <span class="hljs-keyword">return</span> y<br>        <br>    <span class="hljs-comment"># x:输入数据, t:监督数据</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">loss</span>(<span class="hljs-params">self, x, t</span>):</span><br>        y = self.predict(x)<br>        <br>        <span class="hljs-keyword">return</span> cross_entropy_error(y, t)<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">accuracy</span>(<span class="hljs-params">self, x, t</span>):</span><br>        y = self.predict(x)<br>        y = np.argmax(y, axis=<span class="hljs-number">1</span>)<br>        t = np.argmax(t, axis=<span class="hljs-number">1</span>)<br>        <br>        accuracy = np.<span class="hljs-built_in">sum</span>(y == t) / <span class="hljs-built_in">float</span>(x.shape[<span class="hljs-number">0</span>])<br>        <span class="hljs-keyword">return</span> accuracy<br>        <br>    <span class="hljs-comment"># x:输入数据, t:监督数据</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">numerical_gradient</span>(<span class="hljs-params">self, x, t</span>):</span><br>        loss_W = <span class="hljs-keyword">lambda</span> W: self.loss(x, t)<br>        <br>        grads = &#123;&#125;<br>        grads[<span class="hljs-string">&#x27;W1&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="hljs-string">&#x27;W1&#x27;</span>])<br>        grads[<span class="hljs-string">&#x27;b1&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="hljs-string">&#x27;b1&#x27;</span>])<br>        grads[<span class="hljs-string">&#x27;W2&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="hljs-string">&#x27;W2&#x27;</span>])<br>        grads[<span class="hljs-string">&#x27;b2&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="hljs-string">&#x27;b2&#x27;</span>])<br>        <br>        <span class="hljs-keyword">return</span> grads<br></code></pre></div></td></tr></table></figure>

<p>我们接下来在MNIST数据集上(MNIST的图像数据是28像素 <em>×</em> 28像素的灰度图像,各个像素的取值在0到255之间。每个图像数据都相应地标有相应的标签)利用这个两层神经网络实现手写体的识别：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment">###MNIST</span><br><span class="hljs-comment"># 读入数据</span><br><span class="hljs-keyword">from</span> dataset.mnist <span class="hljs-keyword">import</span> load_mnist<br>(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class="hljs-literal">True</span>, one_hot_label=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(x_train.shape)<br><span class="hljs-built_in">print</span>(t_train.shape)<br><span class="hljs-built_in">print</span>(x_test.shape)<br><span class="hljs-built_in">print</span>(t_test.shape)<br><br><br>network = TwoLayerNet(input_size=<span class="hljs-number">784</span>, hidden_size=<span class="hljs-number">50</span>, output_size=<span class="hljs-number">10</span>)<br><br>iters_num = <span class="hljs-number">10000</span>  <span class="hljs-comment"># 适当设定循环的次数</span><br>train_size = x_train.shape[<span class="hljs-number">0</span>]<br>batch_size = <span class="hljs-number">100</span><br>learning_rate = <span class="hljs-number">0.1</span><br><br>train_loss_list = []<br>train_acc_list = []<br>test_acc_list = []<br><br><span class="hljs-comment">##平均每个epoch的重复次数</span><br>iter_per_epoch = <span class="hljs-built_in">max</span>(train_size / batch_size, <span class="hljs-number">1</span>)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iters_num):<br>    batch_mask = np.random.choice(train_size, batch_size)<br>    x_batch = x_train[batch_mask]<br>    t_batch = t_train[batch_mask]<br>    <br>    <span class="hljs-comment"># 计算梯度</span><br>    grad = network.numerical_gradient(x_batch, t_batch)<br>    <br>    <span class="hljs-comment"># 更新参数</span><br>    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> (<span class="hljs-string">&#x27;W1&#x27;</span>, <span class="hljs-string">&#x27;b1&#x27;</span>, <span class="hljs-string">&#x27;W2&#x27;</span>, <span class="hljs-string">&#x27;b2&#x27;</span>):<br>        network.params[key] -= learning_rate * grad[key]<br>    <br>    <span class="hljs-comment">##记录学习过程</span><br>    loss = network.loss(x_batch, t_batch)<br>    train_loss_list.append(loss)<br>    <br>    <span class="hljs-comment">#计算每个epoch的识别精度</span><br>    <span class="hljs-keyword">if</span> i % iter_per_epoch == <span class="hljs-number">0</span>:<br>        train_acc = network.accuracy(x_train, t_train)<br>        test_acc = network.accuracy(x_test, t_test)<br>        train_acc_list.append(train_acc)<br>        test_acc_list.append(test_acc)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;train acc, test acc | &quot;</span> + <span class="hljs-built_in">str</span>(train_acc) + <span class="hljs-string">&quot;, &quot;</span> + <span class="hljs-built_in">str</span>(test_acc))<br><br><span class="hljs-comment"># 绘制图形</span><br>markers = &#123;<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;s&#x27;</span>&#125;<br>x = np.arange(<span class="hljs-built_in">len</span>(train_acc_list))<br>plt.plot(x, train_acc_list, label=<span class="hljs-string">&#x27;train acc&#x27;</span>)<br>plt.plot(x, test_acc_list, label=<span class="hljs-string">&#x27;test acc&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;epochs&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;accuracy&quot;</span>)<br>plt.ylim(<span class="hljs-number">0</span>, <span class="hljs-number">1.0</span>)<br>plt.legend(loc=<span class="hljs-string">&#x27;lower right&#x27;</span>)<br>plt.show()<br></code></pre></div></td></tr></table></figure>

<p>先将所有训练数据随机打乱，然后按指定的批次大小，按序生成mini batch，给每个batch编号，按顺序遍历所有的batch，遍历一次所有的batch称为一个epoch(但是上面的实现是随机选取的)</p>
<h3 id="误差反向传播算法"><a href="#误差反向传播算法" class="headerlink" title="误差反向传播算法"></a>误差反向传播算法</h3><p>上面我们通过数值微分来计算权重参数的梯度，但是计算非常费时间，而误差反向传播算法就是一个可以高效计算权重参数的方法，这里通过<strong>计算图</strong>的方式来理解误差方向传播计算梯度的方法</p>
<p>先来介绍计算图：</p>
<p>问题是：超市买了2个100日元一个的苹果，消费税是10%，请计算支付金额</p>
<p>可以通过计算图来计算这个过程：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111113876.png" srcset="/img/loading.gif" lazyload alt="image-20210103111113876"></p>
<p>箭头上面标上结果，在节点内部进行计算</p>
<p>这种从左到右的计算方向称为正向传播，如果我们现在要计算<em>苹果价格的波动会在多大程度上影响最终的支付金额</em>，也就是要计算最终的支付金额对苹果价格的导数，这个时候就可以从右到左进行计算导数，这个过程就叫<strong>反向传播</strong></p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111122966.png" srcset="/img/loading.gif" lazyload alt="image-20210103111122966"></p>
<p>反向传播的计算顺序为：将传来的信号乘以局部导数再传递给下个节点</p>
<p>反向传播实现的关键是<strong>链式法则</strong>：</p>
<blockquote>
<p>如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示</p>
</blockquote>
<p>举个例子：有一个函数：$z=(x+y)^2$是复合函数，可以拆成两个函数：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111132022.png" srcset="/img/loading.gif" lazyload alt="image-20210103111132022"></p>
<p>那么这个函数对x的偏导数就可以这样求：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111139878.png" srcset="/img/loading.gif" lazyload alt="image-20210103111139878"></p>
<p>这个过程利用计算图求解如下：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111151298.png" srcset="/img/loading.gif" lazyload alt="image-20210103111151298"></p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111206447.png" srcset="/img/loading.gif" lazyload alt="image-20210103111206447"></p>
<p>我们现在基于计算图来实现各个层的反向传播</p>
<p>首先是加法节点：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111217127.png" srcset="/img/loading.gif" lazyload alt="image-20210103111217127"></p>
<p>左边是正向传播，右边是反向传播</p>
<p>由于$\frac{\partial f}{\partial x}和\frac{\partial f}{\partial y}$都等于1，所以加法节点将上游的值<strong>原封不动地输出到下游</strong></p>
<p>然后是乘法节点，考虑$z=xy$：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111230193.png" srcset="/img/loading.gif" lazyload alt="image-20210103111230193"></p>
<p>因此乘法的反向传播会<strong>乘以输入信号的翻转值</strong>，所以在实现乘法的反向传播的时候需要保存正向传播的输入信号</p>
<p>python实现：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment">###</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MulLayer</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        self.x = <span class="hljs-literal">None</span><br>        self.y = <span class="hljs-literal">None</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x, y</span>):</span><br>        self.x = x<br>        self.y = y<br>        out = x * y<br>        <span class="hljs-keyword">return</span> out<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self, dout</span>):</span><br>        dx = dout * self.y <span class="hljs-comment"># 翻转x和y</span><br>        dy = dout * self.x<br>        <span class="hljs-keyword">return</span> dx, dy<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">AddLayer</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">pass</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x, y</span>):</span><br>        out = x + y<br>        <span class="hljs-keyword">return</span> out<br>        <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self, dout</span>):</span><br>        dx = dout * <span class="hljs-number">1</span><br>        dy = dout * <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> dx, dy<br></code></pre></div></td></tr></table></figure>

<p>接下来实现激活函数ReLU层和Sigmoid层</p>
<p><strong>ReLU激活函数</strong>及其导数：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111246475.png" srcset="/img/loading.gif" lazyload alt="image-20210103111246475"></p>
<p>用计算图表示：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111257385.png" srcset="/img/loading.gif" lazyload alt="image-20210103111257385"></p>
<p>ReLU激活函数就像一个开关，正向传播的时候有电流通过就将开关打开，这个时候反向传播电流就会直接通过；如果正向传播时没有电流就把开关关闭，反向传播时就不会有电流通过</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Relu</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        self.mask = <span class="hljs-literal">None</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        self.mask = (x &lt;= <span class="hljs-number">0</span>)<br>        out = x.copy()<br>        out[self.mask] = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">return</span> out<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self, dout</span>):</span><br>        dout[self.mask] = <span class="hljs-number">0</span><br>        dx = dout<br>        <span class="hljs-keyword">return</span> dx<br></code></pre></div></td></tr></table></figure>

<p><strong>sigmoid函数</strong>为：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111308934.png" srcset="/img/loading.gif" lazyload alt="image-20210103111308934"></p>
<p>利用计算图表示为：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111321535.png" srcset="/img/loading.gif" lazyload alt="image-20210103111321535"></p>
<p>这里面出现了几个新的节点：<code>/</code>和<code>exp</code>节点</p>
<p><code>/</code>节点表示的函数为：$y=\frac{1}{x}$ ,导数为：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111330640.png" srcset="/img/loading.gif" lazyload alt="image-20210103111330640"></p>
<p>因此该节点将上游的值乘以正向传播的输出平方后再乘以-1传给下游</p>
<p><code>exp</code>节点表示$y=exp(x)$ 其导数还是其自身：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111338055.png" srcset="/img/loading.gif" lazyload alt="image-20210103111338055"></p>
<p>因此<code>exp</code>节点将上游的值乘以正向传播的输出后传给下游</p>
<p>所以结合前面的<code>+</code>和<code>×</code>节点，sigmoid的导数计算图为：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111349060.png" srcset="/img/loading.gif" lazyload alt="image-20210103111349060"></p>
<img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20201224171928291.png" srcset="/img/loading.gif" lazyload alt="image-20201224171928291" style="zoom:67%;" />

<p>进一步整理得到：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111358136.png" srcset="/img/loading.gif" lazyload alt="image-20210103111358136"></p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Sigmoid</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        self.out = <span class="hljs-literal">None</span><br>        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        out = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br>        self.out = out<br>        <span class="hljs-keyword">return</span> out<br>        <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self, dout</span>):</span><br>        dx = dout * (<span class="hljs-number">1.0</span> - self.out) * self.out<br>        <span class="hljs-keyword">return</span> dx<br></code></pre></div></td></tr></table></figure>

<p>对输入信号进行加权求和的层叫做<strong>Affine层</strong>，可以用如下的计算图来表示：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111408215.png" srcset="/img/loading.gif" lazyload alt="image-20210103111408215"></p>
<p>其中dot表示矩阵相乘</p>
<p>其反向传播过程：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111416858.png" srcset="/img/loading.gif" lazyload alt="image-20210103111416858"></p>
<p>对于1，2可以类比于乘法，再根据矩阵的维度来理解:比如$\frac{\partial (X \cdot W) }{\partial X}$  如果结果是W，那么W的维度为(2,3)而$\frac{\partial L}{\partial Y}$的维度为(3,1),这样就不能相乘，所以结果是W的转置</p>
<p>基于上面的计算图，可以使用python来实现<strong>Affine层</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Affine</span>:</span><br> 	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, W, b</span>):</span><br> 		self.W = W<br> 		self.b = b<br> 		self.x = <span class="hljs-literal">None</span><br> 		self.dW = <span class="hljs-literal">None</span><br> 		self.db = <span class="hljs-literal">None</span><br> 	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br> 		self.x = x<br> 		out = np.dot(x, self.W) + self.b<br> 		<span class="hljs-keyword">return</span> out<br> 	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self, dout</span>):</span><br> 		dx = np.dot(dout, self.W.T)<br> 		self.dW = np.dot(self.x.T, dout)<br> 		self.db = np.<span class="hljs-built_in">sum</span>(dout, axis=<span class="hljs-number">0</span>)<br> 		<span class="hljs-keyword">return</span> dx<br></code></pre></div></td></tr></table></figure>

<p>最后还有一个比较重要的是<strong>Softmax-with-Loss 层</strong> ,也就是包含Softmax函数和交叉熵误差的层</p>
<p>交叉熵误差表示为：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111429635.png" srcset="/img/loading.gif" lazyload alt="image-20210103111429635"></p>
<p>用计算图表示为：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111437312.png" srcset="/img/loading.gif" lazyload alt="image-20210103111437312"></p>
<p>这个反向传播的要注意的就是log节点：</p>
<p>log函数为$y=log(x)$ ,所以其导数为$\frac{\partial y }{\partial x}=\frac{1}{x}$ ，而加节点(原封不动)和乘节点(翻转)之前已经见过了</p>
<p>因此交叉熵误差的反向传播用计算图表示为：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111446952.png" srcset="/img/loading.gif" lazyload alt="image-20210103111446952"></p>
<p>Softmax函数为：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111453904.png" srcset="/img/loading.gif" lazyload alt="image-20210103111453904"></p>
<p>用计算图表示为：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111502001.png" srcset="/img/loading.gif" lazyload alt="image-20210103111502001"></p>
<p>反向传播可以分成几个步骤：</p>
<ul>
<li><p>交叉熵误差层传过来的值</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111512352.png" srcset="/img/loading.gif" lazyload alt="image-20210103111512352"></p>
</li>
<li><p>乘节点将正向传播的值翻转后相乘：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111523824.png" srcset="/img/loading.gif" lazyload alt="image-20210103111523824"></p>
</li>
<li><p>正向传播时若有分支流出，则反向传播时它们的反向传播的值会相加，因此对于除节点反向传播的输入为$-S(t_1+t_2+t_3)$ 然后进行除节点的反向传播:$-S(t_1+t_2+t_3)×(\frac{1}{S})^2=\frac{1}{S}(t_1+t_2+t_3)$ ,这里面t表示训练数据的标签为ont-hot向量(0,1向量)，因此结果为$\frac{1}{S}$，用计算图表示如下：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111536294.png" srcset="/img/loading.gif" lazyload alt="image-20210103111536294"></p>
</li>
<li><p>加节点原封不动的传递，乘节点进行翻转：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111547342.png" srcset="/img/loading.gif" lazyload alt="image-20210103111547342"></p>
</li>
<li><p>exp节点根据前面的推导，将上游的值乘以正向传播的输出后传给下游，也就是$(\frac{1}{S}-\frac{t_1}{exp(a_1)})exp(a_1)$ 整理可得$y_1-t_1$ :</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20201227191132812.png" srcset="/img/loading.gif" lazyload> </p>
</li>
</ul>
<p>将上面的交叉熵误差和softmax总结起来可以用下面的计算图表示：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210103111609941.png" srcset="/img/loading.gif" lazyload alt="image-20210103111609941"></p>
<p>这样就可以使用python进行简单的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SoftmaxWithLoss</span>:</span><br> 	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br> 		self.loss = <span class="hljs-literal">None</span> <span class="hljs-comment"># 损失</span><br> 		self.y = <span class="hljs-literal">None</span> <span class="hljs-comment"># softmax的输出</span><br> 		self.t = <span class="hljs-literal">None</span> <span class="hljs-comment"># 监督数据（one-hot vector）</span><br> 	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x, t</span>):</span><br> 		self.t = t<br> 		self.y = softmax(x)<br> 		self.loss = cross_entropy_error(self.y, self.t)<br> 		<span class="hljs-keyword">return</span> self.loss<br> 	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span>(<span class="hljs-params">self, dout=<span class="hljs-number">1</span></span>):</span><br> 		batch_size = self.t.shape[<span class="hljs-number">0</span>]<br> 		dx = (self.y - self.t) / batch_size<br> 		<span class="hljs-keyword">return</span> dx<br></code></pre></div></td></tr></table></figure>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/reading-notes/">reading notes</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/notes/">notes</a>
                    
                      <a class="hover-with-bg" href="/tags/deep-learning/">deep learning</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2021/01/04/r-inter/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">【R语言编程指南】R内部机制</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div id="waline"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#waline', function() {
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/@waline/client@0.14.8/dist/Waline.min.js', function () {
        new Waline({
          el: "#waline",
          placeholder: "说点什么",
          path: window.location.pathname,
          avatar: "retro",
          meta: ["nick","mail","link"],
          pageSize: "5",
          lang: "zh-CN",
          highlight: true,
          serverURL: "https://comments-flax.vercel.app",
          avatarCDN: "",
          avatarForce: false,
          requiredFields: [],
          emojiCDN: "https://cdn.jsdelivr.net/gh/walinejs/emojis@1.0.0/bilibili",
          emojiMaps: {},
          anonymous: null,
        });
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- LeanCloud 统计PV -->
        <span id="leancloud-site-pv-container" style="display: none">
            总访问量 
            <span id="leancloud-site-pv"></span>
             次
          </span>
      
      
        <!-- LeanCloud 统计UV -->
        <span id="leancloud-site-uv-container" style="display: none">
            总访客数 
            <span id="leancloud-site-uv"></span>
             人
          </span>
      

    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.2/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js" ></script>




  <script defer src="/js/leancloud.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    (function () {
      var path = "/local-search.xml";
      $('#local-search-input').on('click', function() {
        searchFunc(path, 'local-search-input', 'local-search-result');
      });
      $('#modalSearch').on('shown.bs.modal', function() {
        $('#local-search-input').focus();
      });
    })()
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-svg.js" ></script>

  








  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
